---
title: "STAT_5405_HW_6"
output: html_document
date: "2023-10-13"
---

##1a. 

Dataset deforestation, contains the information about the forest cover data belongs to 4 provinces of China. We were trying to figure out the reason behind the rapid deforestation on basis of the 17 predictors to one response variable called $fcover$ by drawing a relataion ship between the explanatory and response variable
```{r}

library(tidyverse)
data1 <- read.csv("Data/deforest.csv", header = TRUE)
data2 <- data1
str(data1)

```
We can observe total of 405 observation in the data set with total of 18 variables where as 17 of them are explanatory variables and one is response variable.

Lets first satisfy the normality and variance assumptions on the response variable so that we can proceed further with the analysis on the data by fitting the multiple linear regression.
```{r}

library(e1071)
hist(data1$fcover, breaks=30, main="", xlab="fcover")
shapiro.test(data1$fcover)
skewness(data1$fcover)

```
We can observe that the data is **not satisfying the normality with the fcover** data as the data is right skewed which was confirmed statistically through the shapiro-wilk test - pvalue(which is less than the significance level).

I have applied log transformation to the fcover variable to check for the normality assumption
```{r}

hist(log(data1$fcover), breaks=30, main="", xlab="fcover")

library(e1071)
shapiro.test(log(data1$fcover))
skewness(log(data1$fcover))

```
From the above we can observe that the **data is right skewed and normality assumption is not satisfied**.

Following is the code to fit the MLR model on top of the log transformed response variable with respective to all the predictor variables. Data was splitted with the train and test data sets on the proportion of 80% and 20% respectively and the data was standardized on the train data by mean. 
```{r}

data1$logfcover <- log(data1$fcover)

set.seed(87654)
train.prop <- 0.80
trnset <- sort(sample(1:nrow(data1), ceiling(nrow(data1)*train.prop)))
# create the training and test sets
train.set <- data1[trnset, ]
test.set  <- data1[-trnset, ]

library(caret)
contpredcols <- 5:20
# Find mean and std dev of train.set.1
normParam <- preProcess(train.set[,contpredcols],
                        method = c("center", "scale"))
# standardize the training set based on its mean and std
data.train <- cbind(train.set[,c("logfcover", "policy")],
                    predict(normParam, train.set[,contpredcols])) 
# standardize test set based on the above mean and std dev of training set
data.test <- cbind(test.set[,c("logfcover", "policy")],
                   predict(normParam, test.set[,5:20]))

mod.log <- lm(logfcover ~., data = data.train)
summary(mod.log)

```
With the extremely small p-values illustrates us the relationship of the response variable with the explanatory variable and the **75% of response variable is explained by the predictor variables** which we can confirmed by the multiple R-squared value and also we can observe the regression co-efficient values of each variable where negative sign indicates that they are negatively co-related and positive indicates that they are positively co-related.

below plot is for the predicted values of the above model that we fitted and we can observe that the data predicted was not normal
```{r}

plot(data.test$logfcover, predict(mod.log, newdata = data.test), 
     col="grey33", cex=0.3, xlab="Actual", ylab="Predicted")
abline(0,1)

```

Following is the code to fit the data for the normality assumption by applying square root
```{r}

hist(sqrt(data1$fcover), breaks=30, main="", xlab="fcover")

library(e1071)
shapiro.test(sqrt(data1$fcover))
skewness(sqrt(data1$fcover))


```
From the above bar graph we can observe that the **data is not normally distributed** however by applying the shapiro-wilk test it rejects the null hypothesis of normally distributed. 

Following is the code to fit the data to multiple linear regression on the sqaure root of the fcover vairable as the response variable

```{r}

data1$sqrtfcover <- sqrt(data1$fcover)

set.seed(87654)
train.prop <- 0.80
trnset <- sort(sample(1:nrow(data1), ceiling(nrow(data1)*train.prop)))
# create the training and test sets
train.set <- data1[trnset, ]
test.set  <- data1[-trnset, ]

library(caret)
contpredcols <- 5:20
# Find mean and std dev of train.set.1
normParam <- preProcess(train.set[,contpredcols],
                        method = c("center", "scale"))
# standardize the training set based on its mean and std
data.train <- cbind(train.set[,c("sqrtfcover", "policy")],
                    predict(normParam, train.set[,contpredcols])) 
# standardize test set based on the above mean and std dev of training set
data.test <- cbind(test.set[,c("sqrtfcover", "policy")],
                   predict(normParam, test.set[,5:20]))

mod.1 <- lm(sqrtfcover ~., data = data.train)
summary(mod.1)

```
With the extremely small p-values illustrates us the relationship of the response variable with the explanatory variable and the 82.41% of response variable is explained by the predictor variables which we can confirmed by the multiple R-squared value and also we can observe the regression co-efficient values of each variable where negative sign indicates that they are negatively co-related and positive indicates that they are positively co-related. 

below plot is for the predicted values of the above model that we fitted and we can observe that the predicted values are normal.
```{r}

plot(data.test$sqrtfcover, predict(mod.1, newdata = data.test), 
     col="grey33", cex=0.3, xlab="Actual", ylab="Predicted")
abline(0,1)

```

From the above two models with the log transformed data and the square root data normality assumption did not satisfy for both of them even though the p-values were same for both it is clear that the **goodness of fit is more with the square rooted response variable with the value of 82.4% when compared to the log transformation with the 74.62%**. However, more the r-squared indicates the over fitting the model where we need to verify other factors such as co-linearity, ouliers, leverages points and further diagnostics on the residuals. Predicted values lies close to the normal line for the square rooted values where as log transformed does not seem to be linear.

1.b.

Following is the code implementation of the box-cox transformation on the response variable.
```{r}


library(MASS)

boxcox.out <- boxcox(lm(fcover ~ 1, data = data2))
(lambda <- boxcox.out$x[which.max(boxcox.out$y)])
data2$boxcoxfcover <- (data2$fcover^lambda -1)/lambda


hist(sqrt(data2$boxcoxfcover), breaks=30, main="", xlab="fcover")

library(e1071)
shapiro.test(data2$boxcoxfcover)
skewness(data2$boxcoxfcover)

```
**Normality assumption on the response variables is satisfied using the box-cox transformation** which we confirmed statistically from above plots and significant test.

```{r}

set.seed(87654)
train.prop <- 0.80
trnset <- sort(sample(1:nrow(data2), ceiling(nrow(data2)*train.prop)))
# create the training and test sets
train.set <- data2[trnset, ]
test.set  <- data2[-trnset, ]

library(caret)
contpredcols <- 5:20
# Find mean and std dev of train.set.1
normParam <- preProcess(train.set[,contpredcols],
                        method = c("center", "scale"))
# standardize the training set based on its mean and std
data.train <- cbind(train.set[,c("boxcoxfcover", "policy")],
                    predict(normParam, train.set[,contpredcols])) 
# standardize test set based on the above mean and std dev of training set
data.test <- cbind(test.set[,c("boxcoxfcover", "policy")],
                   predict(normParam, test.set[,5:20]))

mod.2 <- lm(boxcoxfcover ~., data = data.train)
summary(mod.2)


```
With the extremely small p-values illustrates us the relationship of the response variable with the explanatory variable and the 82.38% of response variable is explained by the predictor variables which we can confirmed by the multiple R-squared value and also we can observe the regression co-efficient values of each variable where negative sign indicates that they are negatively co-related and positive indicates that they are positively co-related. 

Below plot is for the predicted values of the above model that we fitted and we can observe that the predicted values are normal.
```{r}

plot(data.test$boxcoxfcover, predict(mod.2, newdata = data.test), 
     col="grey33", cex=0.3, xlab="Actual", ylab="Predicted")
abline(0,1)

```
2.

Waste dataset contains total of 6 variables where Solidwatse is the response variable whereas the rest are the predictor variables. There are total of 40 observations and indicates the data of each variable contributing to the SolidWaste.

```{r}
library("RobStatTM")
data("waste")
str(waste)
```
```{r}

library(e1071)
hist(waste$SolidWaste, breaks=30, main="", xlab="fcover")
waste$SolidWaster <- as.numeric(waste$SolidWaste)
shapiro.test(waste$SolidWaste)
skewness(waste$SolidWaste)
```
From the above plot we can observe that the data is **not satisfying the normality assumption** on the response variable and was confirmed by the shapiro wilk test.

```{r}

library(e1071)
hist(log(waste$SolidWaste), breaks=30, main="", xlab="fcover")
waste$SolidWaster <- as.numeric(waste$SolidWaste)
shapiro.test(log(waste$SolidWaste))
skewness(log(waste$SolidWaste))

```
From the above plot we can observe that the **data is satisfying the normality assumption on the log transformation** of response variable and was confirmed by the Shapiro wilk test.

```{r}

library(e1071)
hist(sqrt(waste$SolidWaste), breaks=30, main="", xlab="fcover")
waste$SolidWaster <- as.numeric(waste$SolidWaste)
shapiro.test(sqrt(waste$SolidWaste))
skewness(sqrt(waste$SolidWaste))

```
From the above plot we can observe that the data is not satisfying the normality assumption on the square root transformation of response variable and was confirmed by the Shapiro wilk test.


```{r}

waste$SolidWaste <- log(waste$SolidWaste)

mod.waste <- lm(SolidWaste ~ Land + Metals + Trucking + Retail + Restaurants, data = waste)
summary(mod.waste)

```
With the extremely small p-values illustrates us the relationship of the response variable with the explanatory variable and the 68.41% of response variable is explained by the predictor variables which we can confirmed by the multiple R-squared value and also we can observe the regression co-efficient values of each variable where negative sign indicates that they are negatively co-related and positive indicates that they are positively co-related. land, trucking, retail where negatively co related whereas the restaturants and metals were postively co-related.

```{r}
res <- residuals(mod.waste)

car::qqPlot(res, main = NA, pch = 19, col = 2, cex = 0.7)

skew <- skewness(res)
cat("skewness of the residuals -> ", skew)
shapiro.test(res)

```
**Residuals of the model were normally distributed** and we can confirm from the above qq-plot we also statistically confirmed by the Shapiro-wilk test as its p-value accepting the null hypothesis of normality by its p-value greater than the level of significance

2.b.

Below code indicates the studentized residuals 
```{r}

#we need to calcualte the studentised residuals to get the intervals 
sigma_residuals <- sigma(mod.waste)
studentized_residuals <- res / sigma_residuals

plot(fitted(mod.waste), studentized_residuals, xlab = "Fitted Values", ylab = "Studentized Residuals", 
     main = "Studentized Residuals vs. Fitted Values")
abline(h = 0, col = "red")

boxplot(res, main = "Box Plot of Data", ylab = "Variable Name")

extpts <- which(abs(residuals(mod.waste)) > 3*sd(residuals(mod.waste)))

extpts
```
From the above studentized residuals plot we can observe that the most of the data points were lied at the 0 line and in the range of 2, -2 one on of the point was little above to 2 however it is negligible with 95% significance and can confirm that the model is reasonable. From the box plot we can observe two of the outliers whereas after applying the standard deviation 3 times with the residuals with the abs the result is 0 and so we can confirm that there is no effect of the outliers on the model which generally misleads the model.

2.c

Following code figures out the leverage points on the X-axis which are the explanatory variables
```{r}

n <- nrow(waste)
p <- ncol(waste)-1
(hilev <- which(influence(mod.waste)$hat > max(2*(p+1)/n,0.5)))


```

Following code plots the leverage points
```{r}

par(mfrow=c(1,1))
plot(rstandard(mod.waste)^2, influence(mod.waste)$hat, pch =19, cex=0.5, col="blue",
     xlab = "squared residual", ylab = "leverage")
inf0 <- which(influence(mod.waste)$hat > 0.5)
text(rstandard(mod.waste)[hilev]^2, influence(mod.waste)$hat[hilev], 
     labels = inf0, cex = 0.9, font =2, pos =1)

```
from the above plot we can find the leverage points from the 0 line where as the points far from them may show some effect on the model however neither outliers nor leverage points necessarily influential. To know the influence of them on the model we have several other tests such as Cookâ€™s distance, DFFITS, DFBETAS and following code implements them.

```{r}
par(mfrow=c(2,2))
boxplot (rstudent (mod.waste), sub =" Stud . resid ")
boxplot (influence(mod.waste)$hat , sub=" leverages ")
boxplot(cooks.distance(mod.waste), sub = "Cook's D")
boxplot(dffits(mod.waste), sub = "DFFITS")
```
From the above plot of cook's Distance we can observe an outlier which indicates that it is largely effecting the model and by removing it results of the model could change and same applies to the dffits where there is an oulier below to the 0 beyond -2. 

```{r}
dfbetas.all  <- as.data.frame(dfbetas(mod.waste))
par(mfrow = c(2,3))
boxplot(dfbetas.all[,1], sub ="Land")
boxplot(dfbetas.all[,2], sub ="Metals")
boxplot(dfbetas.all[,3], sub ="Trucking")
boxplot(dfbetas.all[,4], sub ="Retail")
boxplot(dfbetas.all[,5], sub ="Restaurants")
```
From the above bfbeats plots we can conclude that the points lies close to the 0 has no significnace on the model and removing them will not effect the results where are the values beyond the 1 will have effect on the model and removing the values can effect the model.

3.

Vif which is popularly known as variance inflation factor indicates the multicolinearity between the explanatory variables where higher than the values of a variable tells us that it is higlhyl corelated with the other predictors and removing it can improve the model fit.
```{r}
library("olsrr")
car::vif(mod.waste) 
(mod.condind <- ols_eigen_cindex(mod.waste)[,2])
(mod.condnum <- max(mod.condind)/min(mod.condind))
```
From the above results, we can observe the values of all the five predictors variables are less than 9.31019 (thresholds can be set the max limit that the VIF value) which indicates all the variables are independent. We can select all the variables to the model of MLR.

Below is the code to implement rigid regression which is the regularized regression or also known as the penalized regression which regularizes the model using the $lambda$ parameter where the 

In ridge regression, we minimize the error sum of squares function 
$S(\beta_1,\ldots, \beta_p)$ subject to a penalty function denoted by 

$$ 
\lambda \parallel \boldsymbol{\beta} \parallel^2 = 
\lambda \sum_{j=1}^p \beta_j^2
$$  {#eq-ridgepenalty}

We need to pass a matrix to the glmnet, lambda values needs to be selected manaually and we have taken 100 for sample and the result is 29.676
```{r}
library(glmnet)
pred.mat <- as.matrix(waste) # predictors
resp <- waste$SolidWaste # response

mod.ridge.1=glmnet(pred.mat, resp, alpha = 0, standardize=FALSE)
(lambda.m  <- mod.ridge.1$lambda[100]) 
coef(mod.ridge.1, s= lambda.m)


```
From the above results we see all the variables as according to the rigid mechanism it takes all the variables of the dataset

The glmnet package allows us to select the best lambda value using K-fold cross-validation, by minimizing a user selected criterion, such as MSE. We use 10-fold validation below.
```{r}
cvfit.ridge <- cv.glmnet(pred.mat,resp,alpha=0, standardize=FALSE, 
                         type.measure = "mse", nfolds = 10)
plot(cvfit.ridge)
```

```{r}
cvfit.ridge$lambda.min
```
By using the 10-fold cross-validation we can observe the min lambda value as 32.56

Following are the intercept values for min lambda value dtermined above
```{r}
coef(cvfit.ridge, s = "lambda.min")
```
Or, we can select the lambda value which gives the most regularized model such that the cross-validated error is within one standard error of the minimum. We obtain the ridge regression coefficients corresponding to lambda.1se:
 
```{r}
cvfit.ridge$lambda.1se
```

```{r}
coef(cvfit.ridge, s = "lambda.1se")
```
Binded both the values of lambda min as well as 1se
```{r}
(all.coef <- cbind(coef(cvfit.ridge, s = "lambda.min"),
                   coef(cvfit.ridge, s = "lambda.1se"))) 
bigmat <- cbind(resp,pred.mat)
mod.ridge.new <- MASS::lm.ridge(resp ~ ., as.data.frame(bigmat))
```
We can interpret the intercepts of the predictors as well as the response variabels 
```{r}
plot(lm.ridge(resp ~ ., as.data.frame(bigmat), lambda = seq(0,2,0.05)))
```